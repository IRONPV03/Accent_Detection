{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UF0FPGk3RRVm",
        "outputId": "a95c6fa5-aa47-4c37-eb31-a8deed1c4f49"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install torchaudio"
      ],
      "metadata": {
        "id": "6Sw--D4OUs06"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# !unzip '/content/drive/MyDrive/accentDetection/Dataset.zip' -d '/content/Dataset'\n",
        "!unzip '/content/drive/MyDrive/Dataset.zip' -d '/content/Dataset'"
      ],
      "metadata": {
        "id": "YpFVXPT5aSRG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wdStErK7Q8z1"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import torch\n",
        "import torchaudio\n",
        "import torchaudio.transforms as T\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import librosa\n",
        "from torch.utils.data import random_split\n",
        "from torch.optim import Adam\n",
        "import torch.nn as nn\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import confusion_matrix, roc_auc_score"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class AudioDataset(Dataset):\n",
        "    def __init__(self, root_dir, data_transform=None, sample_duration=5):\n",
        "        self.root_dir = root_dir\n",
        "        self.data_transform = data_transform\n",
        "        self.classes = sorted(os.listdir(root_dir))\n",
        "        print(self.classes)\n",
        "        self.sample_duration = sample_duration\n",
        "\n",
        "        self.data = []\n",
        "        self.labels = []\n",
        "        for i, class_name in enumerate(self.classes):\n",
        "            class_path = os.path.join(root_dir, class_name)\n",
        "            for filename in os.listdir(class_path):\n",
        "                filepath = os.path.join(class_path, filename)\n",
        "                self.data.append(filepath)\n",
        "                self.labels.append(i)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        waveform, _ = torchaudio.load(self.data[idx], normalize=True)\n",
        "\n",
        "        # Resample to 16 kHz\n",
        "        resample_transform = librosa.resample(y=waveform.numpy(), orig_sr=waveform.size(1), target_sr=16000)\n",
        "        waveform = torch.tensor(resample_transform)\n",
        "\n",
        "        # Trim or pad to ensure consistent length\n",
        "        target_length = 16000 * self.sample_duration\n",
        "        if waveform.size(1) > target_length:\n",
        "            waveform = waveform[:, :target_length]\n",
        "        else:\n",
        "            padding = torch.zeros(1, target_length - waveform.size(1))\n",
        "            waveform = torch.cat([waveform, padding], dim=1)\n",
        "\n",
        "        # Convert PyTorch tensor to NumPy array\n",
        "        waveform_np = waveform.numpy()\n",
        "\n",
        "        # Extract MFCC features\n",
        "        mfccs = self.extract_mfcc(waveform_np)\n",
        "\n",
        "        # Apply data transformations if provided\n",
        "        if self.data_transform:\n",
        "            waveform_np = self.data_transform(waveform_np)\n",
        "            mfccs = self.data_transform(mfccs)\n",
        "\n",
        "        label = torch.tensor(self.labels[idx], dtype=torch.long)\n",
        "        return waveform_np, mfccs, label\n",
        "\n",
        "    @staticmethod\n",
        "    def extract_mfcc(audio, n_mfcc=13):\n",
        "        mfccs = librosa.feature.mfcc(y=audio, sr=16000, n_mfcc=n_mfcc)\n",
        "        return mfccs"
      ],
      "metadata": {
        "id": "yOVwIBk2RVtD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class AccentModel(nn.Module):\n",
        "    def __init__(self, num_classes):\n",
        "        super(AccentModel, self).__init__()\n",
        "\n",
        "        self.waveform_branch = nn.Sequential(\n",
        "            nn.Conv1d(1, 64, kernel_size=3, stride=1, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool1d(kernel_size=2, stride=2),\n",
        "            nn.Conv1d(64, 128, kernel_size=3, stride=1, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool1d(kernel_size=2, stride=2),\n",
        "            nn.Flatten(),\n",
        "            nn.Linear(2560000, 256),\n",
        "            nn.ReLU()\n",
        "        )\n",
        "\n",
        "        self.mfcc_branch = nn.Sequential(\n",
        "            nn.Conv2d(1, 32, kernel_size=3, stride=1, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
        "            nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.AdaptiveAvgPool2d((1, 1)),\n",
        "            nn.Flatten(),\n",
        "            nn.Linear(64, 256),  # The input size is now the number of output channels of the last convolutional layer\n",
        "            nn.ReLU()\n",
        "        )\n",
        "\n",
        "        # Correct the input size of the last linear layer\n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.Linear(256 + 256, 256),  # Updated input size\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(256, num_classes)\n",
        "        )\n",
        "\n",
        "    def forward(self, waveform, mfcc):\n",
        "        waveform_out = self.waveform_branch(waveform)\n",
        "        mfcc_out = self.mfcc_branch(mfcc)\n",
        "\n",
        "        # Concatenate along the correct dimension (dim=1)\n",
        "        out = torch.cat((waveform_out, mfcc_out), dim=1)\n",
        "\n",
        "        out = self.classifier(out)\n",
        "        return out"
      ],
      "metadata": {
        "id": "_phGPBbrTuCw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Assuming you have a validation dataset\n",
        "dataset = AudioDataset(root_dir='/content/Dataset/Dataset')\n",
        "train_size = int(0.7 * len(dataset))\n",
        "eval_size = len(dataset) - train_size\n",
        "train_dataset, test_dataset = random_split(dataset, [train_size, eval_size], generator=torch.Generator().manual_seed(42))\n",
        "# train_dataset, test_dataset = train_test_split(dataset, test_size=0.2, random_state=42)\n",
        "train_dataloader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
        "test_dataloader = DataLoader(test_dataset, batch_size=16, shuffle=True)"
      ],
      "metadata": {
        "id": "mMhE5Y6ZSnLX",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "08244fd1-a2b6-4dac-c84d-9f98c87cb202"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Arabic', 'Chinese', 'Hindi', 'Korean', 'Spanish', 'Vietnamese']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize the model, define loss, and optimizer\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "num_classes = 6\n",
        "model = AccentModel(num_classes).to(device)\n",
        "loss_fn = nn.CrossEntropyLoss()\n",
        "optimizer = Adam(model.parameters())"
      ],
      "metadata": {
        "id": "T5IqDraISx1u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tqdm import tqdm\n",
        "\n",
        "num_epochs = 10\n",
        "for epoch in range(num_epochs):\n",
        "    model.train()\n",
        "    # Create a progress bar\n",
        "    pbar = tqdm(enumerate(train_dataloader), total=len(train_dataloader))\n",
        "    for i, (waveforms, mfccs, labels) in pbar:\n",
        "        # Move data to the correct device\n",
        "        waveforms = waveforms.to(device)\n",
        "        mfccs = mfccs.to(device)\n",
        "        labels = labels.to(device)\n",
        "\n",
        "        # Forward pass\n",
        "        waveform_out = model.waveform_branch(waveforms)\n",
        "        mfcc_out = model.mfcc_branch(mfccs)\n",
        "\n",
        "        # print(\"Intermediate Shapes - Waveform Out:\", waveform_out.shape, \"MFCC Out:\", mfcc_out.shape)\n",
        "\n",
        "        # Concatenate\n",
        "        out = torch.cat((waveform_out, mfcc_out), dim=1)\n",
        "        out = model.classifier(out)\n",
        "        loss = loss_fn(out, labels)\n",
        "\n",
        "        # Backward pass and optimization\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        # Update progress bar\n",
        "        pbar.set_description(f'Epoch [{epoch+1}/{num_epochs}], Step [{i+1}/{len(train_dataloader)}], Loss: {loss.item()}')\n",
        "\n",
        "    total_loss = 0\n",
        "    total_correct = 0\n",
        "\n",
        "    # Don't update parameters while evaluating\n",
        "    with torch.no_grad():\n",
        "        tbar = tqdm(enumerate(test_dataloader), total=len(test_dataloader))\n",
        "        for i, (waveforms, mfccs, labels) in tbar:\n",
        "            # Move data to the correct device\n",
        "            waveforms = waveforms.to(device)\n",
        "            mfccs = mfccs.to(device)\n",
        "            labels = labels.to(device)\n",
        "\n",
        "            # Forward pass\n",
        "            outputs = model(waveforms, mfccs)\n",
        "            loss = loss_fn(outputs, labels)\n",
        "            total_loss += loss.item()\n",
        "\n",
        "            # Compute accuracy\n",
        "            _, predicted = torch.max(outputs, 1)\n",
        "            total_correct += (predicted == labels).sum().item()\n",
        "\n",
        "    # Compute average loss and accuracy\n",
        "    avg_loss = total_loss / len(test_dataloader)\n",
        "    avg_accuracy = total_correct / len(test_dataset)\n",
        "\n",
        "    print(f'Test Loss: {avg_loss}, Test Accuracy: {avg_accuracy * 100:.2f}%')\n",
        "\n",
        "    # Delete variables and empty cache\n",
        "    with torch.no_grad():\n",
        "      del waveforms, mfccs, labels, out\n",
        "    torch.cuda.empty_cache()\n",
        "# print(prof.key_averages().table(sort_by=\"self_cpu_memory_usage\", row_limit=10))"
      ],
      "metadata": {
        "id": "nZRAlkhHS2as",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1fb537ef-87fe-4c33-f0f6-ed7dbb81b34f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch [1/10], Step [292/292], Loss: 0.40087899565696716: 100%|██████████| 292/292 [04:22<00:00,  1.11it/s]\n",
            "100%|██████████| 125/125 [01:22<00:00,  1.52it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test Loss: 0.5023924363851547, Test Accuracy: 79.33%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch [2/10], Step [292/292], Loss: 0.3450455367565155: 100%|██████████| 292/292 [04:20<00:00,  1.12it/s]\n",
            "100%|██████████| 125/125 [01:20<00:00,  1.55it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test Loss: 0.545191270828247, Test Accuracy: 78.63%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch [3/10], Step [292/292], Loss: 0.12581132352352142: 100%|██████████| 292/292 [04:17<00:00,  1.14it/s]\n",
            "100%|██████████| 125/125 [01:19<00:00,  1.58it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test Loss: 0.55018679022789, Test Accuracy: 80.73%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch [4/10], Step [292/292], Loss: 0.013590606860816479: 100%|██████████| 292/292 [04:15<00:00,  1.14it/s]\n",
            "100%|██████████| 125/125 [01:17<00:00,  1.62it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test Loss: 0.5081525136977434, Test Accuracy: 86.04%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch [5/10], Step [292/292], Loss: 0.00047270325012505054: 100%|██████████| 292/292 [04:17<00:00,  1.14it/s]\n",
            "100%|██████████| 125/125 [01:19<00:00,  1.58it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test Loss: 0.5751621939763427, Test Accuracy: 86.14%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch [6/10], Step [292/292], Loss: 1.6887920537556056e-06: 100%|██████████| 292/292 [04:19<00:00,  1.13it/s]\n",
            "100%|██████████| 125/125 [01:19<00:00,  1.57it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test Loss: 0.7204368962366134, Test Accuracy: 85.34%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch [7/10], Step [292/292], Loss: 0.444256454706192: 100%|██████████| 292/292 [04:19<00:00,  1.13it/s]\n",
            "100%|██████████| 125/125 [01:20<00:00,  1.56it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test Loss: 0.825326890796423, Test Accuracy: 81.38%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch [8/10], Step [292/292], Loss: 0.007077077869325876: 100%|██████████| 292/292 [04:19<00:00,  1.12it/s]\n",
            "100%|██████████| 125/125 [01:24<00:00,  1.48it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test Loss: 0.8482066089101136, Test Accuracy: 83.08%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch [9/10], Step [292/292], Loss: 4.150217864662409e-05: 100%|██████████| 292/292 [04:25<00:00,  1.10it/s]\n",
            "100%|██████████| 125/125 [01:22<00:00,  1.52it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test Loss: 0.9860606040507555, Test Accuracy: 83.23%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch [10/10], Step [292/292], Loss: 0.00014793315494898707: 100%|██████████| 292/292 [04:28<00:00,  1.09it/s]\n",
            "100%|██████████| 125/125 [01:22<00:00,  1.52it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test Loss: 1.034877093865536, Test Accuracy: 83.38%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "total_loss = 0\n",
        "total_correct = 0\n",
        "all_predictions = []\n",
        "all_labels = []\n",
        "\n",
        "# Don't update parameters while evaluating\n",
        "with torch.no_grad():\n",
        "    for i, (waveforms, mfccs, labels) in tqdm(enumerate(test_dataloader)):\n",
        "        # Move data to the correct device\n",
        "        waveforms = waveforms.to(device)\n",
        "        mfccs = mfccs.to(device)\n",
        "        labels = labels.to(device)\n",
        "\n",
        "        # Forward pass\n",
        "        outputs = model(waveforms, mfccs)\n",
        "        loss = loss_fn(outputs, labels)\n",
        "        total_loss += loss.item()\n",
        "\n",
        "        # Compute accuracy\n",
        "        _, predicted = torch.max(outputs, 1)\n",
        "        total_correct += (predicted == labels).sum().item()\n",
        "\n",
        "        # Collect predictions and labels for confusion matrix and ROC AUC score\n",
        "        all_predictions.extend(predicted.cpu().numpy())\n",
        "        all_labels.extend(labels.cpu().numpy())\n",
        "\n",
        "# Compute average loss and accuracy\n",
        "avg_loss = total_loss / len(test_dataloader)\n",
        "avg_accuracy = total_correct / len(test_dataset)\n",
        "\n",
        "# Confusion Matrix\n",
        "conf_matrix = confusion_matrix(all_labels, all_predictions)\n",
        "print(\"Confusion Matrix:\")\n",
        "print(conf_matrix)\n",
        "print(f'Test Loss: {avg_loss}, Test Accuracy: {avg_accuracy * 100}%')"
      ],
      "metadata": {
        "id": "lIxNFfnAAjdY",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "98e736a2-307c-4def-d0fa-829f618ce8aa"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "125it [01:21,  1.53it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Confusion Matrix:\n",
            "[[293   3   3   1   7  12]\n",
            " [  8 242   0   2  88  11]\n",
            " [ 26   1 298  12   2   1]\n",
            " [ 16   8   2 284   8  14]\n",
            " [ 17  60   2   0 220  12]\n",
            " [  7   1   1   2   5 329]]\n",
            "Test Loss: 1.0354408485516906, Test Accuracy: 83.38338338338338%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Save or use the trained model for inference\n",
        "torch.save(model.state_dict(), \"/content/drive/MyDrive/audio_classifier.pth\")"
      ],
      "metadata": {
        "id": "QEYcpxE8S73-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "8nbzK_2FtP17"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "HJ5zW2uFsVET"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}